# -*- coding: utf-8 -*-
"""IMDB CNN-BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O9Gw3vN4VZY7nNCV2dv_s7WYCFxqWZer

**Implementation of Sentiment Analysis with BERT+CNN in Tensorflow**

I am going to use a pretreined BERT to compute vector-space representations of a IMDB dataset which has two class
 
 1- **Positive:1**

 2- **Negative:0**
 
This notebook has two section. At the **First part** I will: 

*   Load the IMDB dataset
*   Load a **BERT models** from TensorFlow Hub 
*   Build the model by combining BERT with a classifier
*   Train the model, fine-tuning BERT as part of that
*   Save your model and use it to classify sentences

**Second part** includes using the pretreined BERT to compute vector-space representations of the IMDB dataset to feed to **CNN** downsteam Archtectures.

# **First section**

**About BERT**

BERT (article link [link text](https://arxiv.org/abs/1810.04805)) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.


![image.png](data:image/png;base64,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)



Source: http://www.d2l.ai/chapter_natural-language-processing-pretraining/index.html

**Installing dependencies and importing packages**
"""

!pip install 'tensorflow==2.8.0'

pip install 'tf-estimator-nightly==2.8.0.dev2021122109'

!pip install transformers

# A dependency of the preprocessing for BERT inputs
!pip install tensorflow-text

!pip install tf-models-official==2.7.0

import os
import shutil
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from scipy.stats import spearmanr

from sklearn.model_selection import GroupKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix


import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from tensorflow import keras
from tensorflow.keras import layers


from official.modeling import tf_utils
from official import nlp
from official.nlp import bert

# Load the required submodules


import official.nlp.optimization
import official.nlp.bert.bert_models
import official.nlp.bert.configs
import official.nlp.bert.run_classifier
import official.nlp.bert.tokenization
import official.nlp.data.classifier_data_lib
import official.nlp.modeling.losses
import official.nlp.modeling.models
import official.nlp.modeling.networks




from tqdm.notebook import tqdm

import math
from math import floor, ceil

from transformers import *

np.set_printoptions(suppress=True)
print(tf.__version__)

"""**Loading and preparing the dataset**"""

df = pd.read_csv('/content/IMDBMini Dataset.csv')

nRowsRead = None # specify 'None' if want to read whole file
# labeled_data.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows
df0 = pd.read_csv('/content/IMDBMini Dataset.csv', delimiter=',', nrows = nRowsRead)
df0.dataframeName = 'IMDB Dataset.csv'
nRow, nCol = df0.shape
print('There are {} rows and {} columns'.format(nRow, nCol))

df0.head(5)

#Doing some adjustments
c=df0['sentiment']
df0.rename(columns={'review' : 'text',
                   'sentiment' : 'category'}, 
                    inplace=True)
a=df0['text']
b=df0['category'].map({'positive': 1, 'negative': 0})

df= pd.concat([a,b,c], axis=1)
df

df.rename(columns={'sentiment' : 'category',
                   'category':'label'}, 
                    inplace=True)
df

# Grouping data by label
df.groupby('label').count()

"""**Splitting the data between train, validation and test sets:**"""

X_train_, X_test, y_train_, y_test = train_test_split(
    df.index.values,
    df.label.values,
    test_size=0.10,
    random_state=42,
    stratify=df.label.values,    
)

X_train, X_val, y_train, y_val = train_test_split(
    df.loc[X_train_].index.values,
    df.loc[X_train_].label.values,
    test_size=0.10,
    random_state=42,
    stratify=df.loc[X_train_].label.values,  
)

df['data_type'] = ['not_set']*df.shape[0]
df.loc[X_train, 'data_type'] = 'train'
df.loc[X_val, 'data_type'] = 'val'
df.loc[X_test, 'data_type'] = 'test'

df.groupby(['category', 'label', 'data_type']).count()

df

df_train = df.loc[df["data_type"]=="train"]
df_train.head(5)

df_val = df.loc[df["data_type"]=="val"]
df_val.head(5)

df_test = df.loc[df["data_type"]=="test"]
df_test.head(5)

df.dtypes

""" **Showing the more frequent words in each class using Wordcloud**"""

from wordcloud import WordCloud, STOPWORDS
stopwords = set(STOPWORDS)
stopwords.add("RT")

print(type(STOPWORDS))

import random

def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):
    h = 344
    s = int(100.0 * 255.0 / 255.0)
    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)
    return "hsl({}, {}%, {}%)".format(h, s, l)

wordcloud = WordCloud(
                          background_color='white',
                          stopwords=stopwords,
                          max_words=200,
                          max_font_size=60, 
                          random_state=42
                         ).generate(str(df.loc[df["category"]=="positive"].text))
print(wordcloud)
fig = plt.figure(1)
plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),
           interpolation="bilinear")
plt.axis('off')
plt.show()

def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):
    h = 20
    s = int(100.0 * 255.0 / 255.0)
    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)
    return "hsl({}, {}%, {}%)".format(h, s, l)

wordcloud = WordCloud(
                          background_color='white',
                          stopwords=stopwords,
                          max_words=200,
                          max_font_size=60, 
                          random_state=42
                         ).generate(str((df.loc[df["category"]=="negative"].text)))
print(wordcloud)
fig = plt.figure(1)
plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),
           interpolation="bilinear")
plt.axis('off')
plt.show()

"""**Build TensorFlow input**

[Reference ](https://www.tensorflow.org/guide/data)
"""

train_ds = tf.data.Dataset.from_tensor_slices((df_train.text.values, df_train.label.values))
val_ds = tf.data.Dataset.from_tensor_slices((df_val.text.values, df_val.label.values))
test_ds = tf.data.Dataset.from_tensor_slices((df_test.text.values, df_test.label.values))

"""While **tf.data** tries to propagate shape information, the default settings of Dataset.batch result is an unknown batch size because the last batch may not be full. Note the Nones in the shape

"""

train_ds

"""Use the drop_remainder argument to ignore that last batch, and get full shape propagation"""

train_ds = train_ds.shuffle(len(df_train)).batch(32, drop_remainder=False)
train_ds

val_ds = val_ds.shuffle(len(df_val)).batch(32, drop_remainder=False)
val_ds

test_ds = test_ds.shuffle(len(df_test)).batch(32, drop_remainder=False)
test_ds

"""**Loading models from TensorFlow Hub**

This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow/models/official/nlp/bert which L is used as a number of hidden layers and H as hidden size of H, and A as attention heads.

Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.

[BERT-Base, Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.

[The Small BERT models](https://tfhub.dev/google/collections/bert/1) are instances of the original BERT architecture with a smaller number L of layers (i.e., residual blocks) combined with a smaller hidden size H and a matching smaller number A of attention heads.

[ALBERT: ](https://tfhub.dev/google/collections/albert/1)four different sizes of "A Lite BERT" that reduces model size (but not computation time) by sharing parameters between layers.

[BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.

[Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).

The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose one of the classic BERT sizes or their recent refinements like Electra.
"""

#@title Choose a BERT model to fine-tune

bert_model_name = 'albert_en_base'  #@param ["bert_en_uncased_L-12_H-768_A-12", "bert_en_cased_L-12_H-768_A-12", "bert_multi_cased_L-12_H-768_A-12", "small_bert/bert_en_uncased_L-2_H-128_A-2", "small_bert/bert_en_uncased_L-2_H-256_A-4", "small_bert/bert_en_uncased_L-2_H-512_A-8", "small_bert/bert_en_uncased_L-2_H-768_A-12", "small_bert/bert_en_uncased_L-4_H-128_A-2", "small_bert/bert_en_uncased_L-4_H-256_A-4", "small_bert/bert_en_uncased_L-4_H-512_A-8", "small_bert/bert_en_uncased_L-4_H-768_A-12", "small_bert/bert_en_uncased_L-6_H-128_A-2", "small_bert/bert_en_uncased_L-6_H-256_A-4", "small_bert/bert_en_uncased_L-6_H-512_A-8", "small_bert/bert_en_uncased_L-6_H-768_A-12", "small_bert/bert_en_uncased_L-8_H-128_A-2", "small_bert/bert_en_uncased_L-8_H-256_A-4", "small_bert/bert_en_uncased_L-8_H-512_A-8", "small_bert/bert_en_uncased_L-8_H-768_A-12", "small_bert/bert_en_uncased_L-10_H-128_A-2", "small_bert/bert_en_uncased_L-10_H-256_A-4", "small_bert/bert_en_uncased_L-10_H-512_A-8", "small_bert/bert_en_uncased_L-10_H-768_A-12", "small_bert/bert_en_uncased_L-12_H-128_A-2", "small_bert/bert_en_uncased_L-12_H-256_A-4", "small_bert/bert_en_uncased_L-12_H-512_A-8", "small_bert/bert_en_uncased_L-12_H-768_A-12", "albert_en_base", "electra_small", "electra_base", "experts_pubmed", "experts_wiki_books", "talking-heads_base"]

map_name_to_handle = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_base/2',
    'electra_small':
        'https://tfhub.dev/google/electra_small/2',
    'electra_base':
        'https://tfhub.dev/google/electra_base/2',    
  
}

map_model_to_preprocess = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',
    'electra_small':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'electra_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
}

tfhub_handle_encoder = map_name_to_handle[bert_model_name]
tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')

"""**The preprocessing model**

Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. **TensorFlow Hub** provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.

So, the preprocessing model will be loaded into a **hub.KerasLayer** to compose the fine-tuned model. 
"""

bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)

"""**EX)** Let's try the preprocessing model on some text and see the output:"""

text_test = ['Some films just simply should not be remade']
text_preprocessed = bert_preprocess_model(text_test)

print(f'Keys       : {list(text_preprocessed.keys())}')
print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')

"""As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use

**(input_words_id, input_mask and input_type_ids)**

The input is truncated to **128 tokens**. BERT has a constraint on the maximum length of a sequence after tokenizing. For any BERT model, the maximum sequence length after tokenization is 512.

**The input_type_ids** only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.

**The input masks** is the mask of the words in a sentence – all masks starts with CLF token and SEP token.So the masked array is of size sentence size + 2. The token is defined for CLF – 101 and SEP – 102 and .Also the input mask is 1 for useful tokens,and 0 for padding. 

**The input_words_ids** has the token ids of the input sequences and gives unique id’s for individual words. Each word is encoded(ids can be from a vocabulary), padded and separated Length.
"""

#text_preprocessed

"""To generate embedding now all we need to do is to pass this text_preprocessed to the encoder. """

bert_model = hub.KerasLayer(tfhub_handle_encoder)

bert_results = bert_model(text_preprocessed)

print(f'Loaded BERT: {tfhub_handle_encoder}')
print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')

"""The BERT models return a map with 3 important keys: **pooled_output, sequence_output, encoder_outputs:**

**pooled_output** represents each input sequence as a whole. The shape is **[batch_size, H]**. You can think of this as an embedding for the entire movie review.

**sequence_output** represents each input token in the context. The shape is **[batch_size, seq_length, H**]. You can think of this as a contextual embedding for every token in the movie review.

**encoder_outputs** are the intermediate activations of the L Transformer blocks. outputs["encoder_outputs"][i] is a Tensor of shape **[batch_size, seq_length, 1024]** with the outputs of the i-th Transformer block, for 0 <= i < L. The last value of the list is equal to sequence_output.
"""

#bert_results

"""**Define the model**

The fine-tuned model will be created with the preprocessing model, and the selected BERT model, one Dense and a Dropout layer.
"""

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)

"""Let's check that the model runs with the output of the preprocessing model."""

classifier_model = build_classifier_model()
bert_raw_result = classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))

"""tf.sigmoid Computes sigmoid of x element-wiseand it measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive.If a positive number is large, then its sigmoid will approach to 1 since the formula If a negative number is large, its sigmoid will approach to 0. [Reference](https://www.tensorflow.org/api_docs/python/tf/math/sigmoid) """

tf.keras.utils.plot_model(classifier_model)

"""**Model training**

I now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier.

**Loss function**

Since this is a binary classification problem and the model outputs a probability (a single-unit layer), I'll use losses.BinaryCrossentropy loss function.
"""

loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()

"""**Optimizer**

For fine-tuning, let's use the same optimizer that BERT was originally trained with: the "Adaptive Moments" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as AdamW.

For the learning rate (init_lr), I will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5).
"""

from official.nlp import optimization

epochs = 5
steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

"""**Loading the BERT model and training**"""

classifier_model.compile(optimizer=optimizer,
                         loss=loss,
                         metrics=metrics)

print(f'Training model with {tfhub_handle_encoder}')
history = classifier_model.fit(x=train_ds,
                               validation_data=val_ds,
                               epochs=epochs)

"""**Evaluate the model**
Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy.
"""

loss, accuracy = classifier_model.evaluate(test_ds)

print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

"""**Plot the accuracy and loss over time**

Based on the History object returned by model.fit() the training and validation loss can be plotted for comparison, as well as the training and validation accuracy.

In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy.
"""

history_dict = history.history
print(history_dict.keys())

acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()

plt.subplot(2, 1, 1)
# r is for "solid red line"
plt.plot(epochs, loss, 'r', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
# plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

"""**Export for inference**

Now the fine-tuned model will be saved for later use.
"""

dataset_name = 'imdb'
saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))

classifier_model.save(saved_model_path, include_optimizer=False)

"""Let's reload the model, so you can try it side by side with the model that is still in memory."""

reloaded_model = tf.saved_model.load(saved_model_path)

"""Here you can test the model on any sentence you want, just add to the examples variable below."""

def print_my_examples(inputs, results):
  result_for_printing = \
    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'
                         for i in range(len(inputs))]
  print(*result_for_printing, sep='\n')
  print()


examples = [
    'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy..',
    'Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it''s not preachy or boring..',
    'Encouraged by the positive comments about this film on here I was looking forward to watching this film..',
    'This a fantastic movie of three prisoners who become famous..',
    'Some films just simply should not be remade..'
]

reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))
original_results = tf.sigmoid(classifier_model(tf.constant(examples)))

print('Results from the saved model:')
print_my_examples(examples, reloaded_results)
print('Results from the model in memory:')
print_my_examples(examples, original_results)

"""#**Second part**

**BERT + CNN Model**

For the implementation with CNN, I am using the sequence_output as input to the convolutional layer. It represents each input token in the context. **The shape is [batch_size, seq_length, H].** You can think of this as a contextual embedding for every token in the text. This outputs saves positional information about the inputs, then it would male cense to feed a convolutional layer.
"""

def build_CNN_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    #net = outputs['pooled_output'] # [batch_size, 768].
    net = sequence_output = outputs["sequence_output"] # [batch_size, seq_length, 768]
      
    
    net = tf.keras.layers.Conv1D(32, (2), activation='relu')(net)
    #net = tf.keras.layers.MaxPooling1D(2)(net)
    
    net = tf.keras.layers.Conv1D(64, (2), activation='relu')(net)
    #net = tf.keras.layers.MaxPooling1D(2)(net)
    net = tf.keras.layers.GlobalMaxPool1D()(net)
    
#    net = tf.keras.layers.Flatten()(net)
    
    net = tf.keras.layers.Dense(512, activation="relu")(net)
    
    net = tf.keras.layers.Dropout(0.1)(net)
#   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
    net = tf.keras.layers.Dense(3, activation="softmax", name='classifier')(net)
    
    return tf.keras.Model(text_input, net)

"""**ReLU** stands for rectified linear unit, and is a type of activation function. Mathematically, it is defined as y = max(0, x). It is linear (identity) for all positive values, and zero for all negative values.

Also, **tf.keras.layers.Conv1D** creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. [Reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) 

**Tf.keras.layers.GlobalMaxPool1D()** downsamples the input representation by taking the maximum value over the time dimension.[Reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D)


The **Dropout** layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.[Reference ](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)


**Dense** implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense.[Reference ](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)
"""

cnn_classifier_model = build_CNN_classifier_model()
bert_raw_result = cnn_classifier_model(tf.constant(text_test))
print(tf.sigmoid(bert_raw_result))

tf.keras.utils.plot_model(cnn_classifier_model)

cnn_classifier_model.summary()

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
#metrics = tf.metrics.CategoricalCrossentropy()
#metrics = tf.metrics.Accuracy()

epochs = 10
steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

cnn_classifier_model.compile(optimizer=optimizer,
                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                          metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy'))

"""Defining the weights for BERT+CNN the model."""

#This is an balanced dataset.
positive, negative = np.bincount(df['label'])
total = positive + negative 
print('Examples:\n    Total: {}\n    positive: {} ({:.2f}% of total)\n'.format(
    total, positive, 100 * positive / total))
print('Examples:\n    Total: {}\n    negative: {} ({:.2f}% of total)\n'.format(
    total, negative, 100 * negative / total))

weight_for_0 = (1 /positive)*(total)/2.0 
weight_for_1 = (1 / negative)*(total)/2.0



class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

print(f'Training model with {tfhub_handle_encoder}')
cnn_history = cnn_classifier_model.fit(x=train_ds,
                                       validation_data=val_ds,
                                       epochs=epochs,
                                       class_weight=class_weight
                                       
                                      )

loss, accuracy = cnn_classifier_model.evaluate(test_ds)

print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

history_dict = cnn_history.history
print(history_dict.keys())

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
# acc = history_dict['binary_accuracy']
# val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(12, 10))
fig.tight_layout()

plt.subplot(2, 1, 1)
# "bo" is for "blue dot"
plt.plot(epochs, loss, 'r', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
# plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

result =  cnn_classifier_model.predict(test_ds)
print(result.shape)

"""**Export for inference**
Now you just save your fine-tuned model for later use

"""

dataset_name = 'cnn_hate_speech'
saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))

cnn_classifier_model.save(saved_model_path, include_optimizer=False)

reloaded_model = tf.saved_model.load(saved_model_path)

"""**Results for CNN**"""

result =  cnn_classifier_model.predict(test_ds)
print(result.shape)

result